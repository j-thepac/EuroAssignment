{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.utils import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark import StorageLevel\n",
    "import util\n",
    "from config import *\n",
    "import logging\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "paths:Paths=Paths(\n",
    "    dataLake=\"../DataLake/\"\n",
    "    ,srcSearches=\"../searches/\"\n",
    "    ,srcVisitors=\"../visitors/\"\n",
    "    ,rawSearches=\"../DataLake/raw/searches\"\n",
    "    ,rawVisitors=\"../DataLake/raw/visitors\"\n",
    "    ,ezSearches=\"../DataLake/ez/searches\"\n",
    "    ,ezVisitors=\"../DataLake/ez/visitors\"\n",
    "    ,archive=\"../archive/\"\n",
    "    ,archiveSearches=\"../archive/searches\"\n",
    "    ,archiveVisitors=\"../archive/visitors\"\n",
    ")\n",
    "util.rawZoneSetup(paths)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1: Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/09/09 07:40:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark=SparkSession\\\n",
    ".builder\\\n",
    ".appName(\"test\")\\\n",
    ".getOrCreate()\n",
    "\n",
    "spark.sparkContext.setCheckpointDir(\"../sparkCache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def dataIngestion(srcFolder,targetFolder,archiveFolder):\n",
    "    for f in os.listdir(srcFolder):\n",
    "        srcFile=f\"{srcFolder}/{f}\"\n",
    "        if (\".json\" not in f):os.remove(srcFile)\n",
    "        else:\n",
    "            ts=util.getTsFromFileName(f)\n",
    "            targetPath=f\"{targetFolder}/{ts}/\"\n",
    "            spark.read.json(f\"{srcFolder}/{f}\").coalesce(1).write.mode(\"append\").options(header=\"True\",compression=\"snappy\").parquet(targetPath)\n",
    "            os.rename(srcFile,f\"{archiveFolder}/{f}\")\n",
    "            # logging.info(f\"Completed dataIngestion {srcFile}\")\n",
    "\n",
    "\n",
    "dataIngestion(paths.srcSearches,paths.rawSearches,paths.archiveSearches)\n",
    "dataIngestion(paths.srcVisitors,paths.rawVisitors,paths.archiveVisitors)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2: Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanVisitor(df:DataFrame)->DataFrame:\n",
    "    df=df\\\n",
    "    .withColumn(\"hits_avg\",df[\"hits_avg\"].cast(IntegerType()))\\\n",
    "    .withColumn(\"logged_in\",df[\"logged_in\"].cast(BooleanType()))\\\n",
    "    .withColumn(\"visit_start\", udateHandler(df.visit_start) )\\\n",
    "    .withColumn(\"visits\",df.visits.cast(IntegerType()))\\\n",
    "    .withColumn(\"visitor_id\",trim(df.visitor_id.cast(StringType())))\n",
    "\n",
    "    df=df.withColumn(\"visit_start\", to_timestamp(df.visit_start, \"yyyy-MM-dd HH:mm:ss\"))\n",
    "    df=df.withColumn(\"date\", date_format(df.visit_start,\"yyyy-MM-dd\").cast(DateType()))\\\n",
    "    .na.fill(\"na\",[\"visitor_id\"])\n",
    "    return df\n",
    "\n",
    "def cleanSearches(df:DataFrame)->DataFrame:\n",
    "    df= df\\\n",
    "    .withColumn(\"date_time\",to_timestamp(df.date_time, \"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\"))\\\n",
    "    .withColumn(\"visitor_id\",trim(df.visitor_id.cast(StringType())))\n",
    "    \n",
    "    df=df\\\n",
    "    .withColumn(\"date\", date_format(df.date_time,\"yyyy-MM-dd\").cast(DateType()))\\\n",
    "    .na.fill(\"na\",[\"visitor_id\"])\n",
    "    \n",
    "    return df\n",
    "\n",
    "def dateHandler(dateStr:str)->str:\n",
    "    year=dateStr.split(\"-\")[0]\n",
    "    if(len(year)==2):\n",
    "        dateStr=\"20\"+dateStr\n",
    "    return dateStr\n",
    "\n",
    "udateHandler=udf(dateHandler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root: rawVisitorPartitions = 1 , rawVisitorDF.count=9999 \n",
      "INFO:root: rawSearchesPartitions=1 , rawSearchesPartitions =13200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[date_time: string, destination_out: string, destination_ret: string, flight_date_inbound: string, flight_date_outbound: string, origin_out: string, origin_ret: string, segments: bigint, visitor_id: double]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawVisitorDF= spark.read.options(header=\"True\").options(inferSchema=\"True\").parquet(f\"{paths.rawVisitors}/*\").cache()\n",
    "logging.info(f\" rawVisitorPartitions = {rawVisitorDF.rdd.getNumPartitions()} , rawVisitorDF.count={rawVisitorDF.count()} \")\n",
    "cleanVisitorDF=rawVisitorDF.transform(cleanVisitor)\n",
    "rawVisitorDF.unpersist()\n",
    "\n",
    "rawSearchesDF= spark.read.options(header=\"True\").options(inferSchema=\"True\").parquet(f\"{paths.rawSearches}/*\").cache()\n",
    "logging.info(f\" rawSearchesPartitions={rawSearchesDF.rdd.getNumPartitions()} , rawSearchesPartitions ={rawSearchesDF.count()}\")\n",
    "cleanSearchesDF=rawSearchesDF.transform(cleanSearches)\n",
    "rawSearchesDF.unpersist()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VisitorDimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "visitorDimension=cleanVisitorDF.select(\"visitor_id\").distinct().withColumn(\"visitorkey\",monotonically_increasing_id())\n",
    "lastKey=visitorDimension.select(max(visitorDimension.visitorkey).alias(\"max\")).first()[\"max\"]\n",
    "\n",
    "#Getting new IDs in Searches which is not there in Visitor \n",
    "visitorDimension=cleanSearchesDF\\\n",
    ".select(\"visitor_id\")\\\n",
    ".distinct()\\\n",
    ".join(visitorDimension,[\"visitor_id\"],\"left_anti\")\\\n",
    ".select(\"visitor_id\")\\\n",
    ".withColumn(\"visitorkey\",monotonically_increasing_id()+lastKey+1)\\\n",
    ".union(visitorDimension)\\\n",
    ".persist(storageLevel=StorageLevel.MEMORY_AND_DISK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[visitor_id: string, date_time: timestamp, destination_out: string, destination_ret: string, flight_date_inbound: string, flight_date_outbound: string, origin_out: string, origin_ret: string, segments: bigint, date: date, visitorkey: bigint]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "factVisitor=cleanVisitorDF.join(visitorDimension,[\"visitor_id\"],\"left_outer\").persist(storageLevel=StorageLevel.MEMORY_AND_DISK)\n",
    "factSearches=cleanSearchesDF.join(visitorDimension,[\"visitor_id\"],\"left_outer\").persist(storageLevel=StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "# factVisitor=factVisitor.repartitionByRange(20,factVisitor.visitorkey)\n",
    "# factSearches=factSearches.repartitionByRange(20,factSearches.visitorkey)\n",
    "\n",
    "factVisitor.checkpoint()\n",
    "factSearches.checkpoint()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PeriodDimension (Date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "peridDimension=cleanVisitorDF.select(\"date\").distinct().withColumn(\"datekey\",monotonically_increasing_id())\n",
    "maxPeriodKey=peridDimension.select(max(peridDimension.datekey).alias(\"max\")).first()[\"max\"]\n",
    "\n",
    "peridDimension=cleanSearchesDF\\\n",
    ".select(\"date\")\\\n",
    ".distinct()\\\n",
    ".join(peridDimension,[\"date\"],\"left_anti\")\\\n",
    ".select(\"date\")\\\n",
    ".distinct()\\\n",
    ".withColumn(\"datekey\",maxPeriodKey+monotonically_increasing_id()+1)\\\n",
    ".union(peridDimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[date: date, visitor_id: string, date_time: timestamp, destination_out: string, destination_ret: string, flight_date_inbound: string, flight_date_outbound: string, origin_out: string, origin_ret: string, segments: bigint, visitorkey: bigint, datekey: bigint]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "factVisitor=factVisitor.join(peridDimension,[\"date\"],\"left_outer\").persist(storageLevel=StorageLevel.MEMORY_AND_DISK)\n",
    "factSearches=factSearches.join(peridDimension,[\"date\"],\"left_outer\").persist(storageLevel=StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "factVisitor.checkpoint()\n",
    "factSearches.checkpoint()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation to Verify Count source and intermitten result\n",
    "assert(rawVisitorDF.count() == factVisitor.count() )\n",
    "assert(rawSearchesDF.count() == factSearches.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Task3: Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without Period Dim\n",
    "# factVisitorGrouped=factVisitor.groupBy(\"visitorkey\",\"date\",factVisitor.datekey).agg(max(\"visit_start\").alias(\"visit_start\")).cache()\n",
    "# factVisitorExtended=factVisitorGrouped\\\n",
    "#                         .join(factVisitor,[\"visitorkey\",\"visit_start\"])\\\n",
    "#                         .select(\"visitorkey\",\"visit_start\",factVisitorGrouped.date,factVisitorGrouped.datekey,\"country\",\"region\")\\\n",
    "#                         .withColumnRenamed(\"visit_start\",\"date_time\")\n",
    "\n",
    "\n",
    "# With Period Dimesion\n",
    "factVisitorGrouped=factVisitor.groupBy(\"visitorkey\",factVisitor.datekey).agg(max(\"visit_start\").alias(\"visit_start\")).cache()\n",
    "factVisitorExtended=factVisitorGrouped\\\n",
    "                        .join(factVisitor,[\"visitorkey\",\"visit_start\"])\\\n",
    "                        .select(\"visitorkey\",\"visit_start\",factVisitorGrouped.datekey,\"country\",\"region\")\\\n",
    "                        .withColumnRenamed(\"visit_start\",\"date_time\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Without Period Dim\n",
    "# result = factSearches\\\n",
    "# .join(factVisitorExtended,[\"visitorkey\",\"date\"],\"left_outer\")\\\n",
    "# .select(\"country\",\"region\",\"visitorkey\",\"date\")\\\n",
    "# .groupBy(\"date\",\"country\",\"region\")\\\n",
    "# .agg(count(\"*\").alias(\"count\")).cache()\n",
    "\n",
    "## With Period Dim\n",
    "result = factSearches\\\n",
    ".join(factVisitorExtended,[\"visitorkey\",\"datekey\"],\"left_outer\")\\\n",
    ".select(\"country\",\"region\",\"visitorkey\",factSearches.date)\\\n",
    ".groupBy(\"date\",\"country\",\"region\")\\\n",
    ".agg(count(\"*\").alias(\"count\")).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+------+-----+\n",
      "|      date|country|region|count|\n",
      "+----------+-------+------+-----+\n",
      "|2021-01-27|   null|  null|  109|\n",
      "|2021-03-05|   null|  null|   89|\n",
      "|2021-05-08|   null|  null|  113|\n",
      "|2021-05-02|   null|  null|  108|\n",
      "|2021-04-12|   null|  null|   99|\n",
      "+----------+-------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sampleDate=\"(cast('2021-03-05' as date),cast('2021-04-12' as date),cast('2021-01-27' as date),cast('2021-05-02' as date),cast('2021-05-08' as date) )\"\n",
    "result.filter(f\"date in {sampleDate}\").show()\n",
    "# result.filter(f\"date = cast('2021-04-12' as date)\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# country and region not Found  - but should be esp , co\n",
    "# cleanVisitorDF.filter(f\"date = cast('2021-04-12' as date)\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
