{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.utils import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import util\n",
    "from config import *\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "paths:Paths=Paths(\n",
    "    dataLake=\"../DataLake/\"\n",
    "    ,srcSearches=\"../searches/\"\n",
    "    ,srcVisitors=\"../visitors/\"\n",
    "    ,rawSearches=\"../DataLake/raw/searches/\"\n",
    "    ,rawVisitors=\"../DataLake/raw/visitors/\"\n",
    "    ,ezSearches=\"../DataLake/ez/searches/\"\n",
    "    ,ezVisitors=\"../DataLake/ez/visitors/\"\n",
    ")\n",
    "util.rawZoneSetup(paths)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TASK 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:getNewFiles(): path ../searches/, new Files Count 2\n",
      "INFO:root:getNewFiles(): path ../visitors/, new Files Count 2\n"
     ]
    }
   ],
   "source": [
    "newSearchesFiles=util.getNewFiles(paths.srcSearches,paths.rawSearches)\n",
    "newVisitorFiles=util.getNewFiles(paths.srcVisitors,paths.rawVisitors)\n",
    "if(len(newSearchesFiles)==0 and len(newVisitorFiles)==0):\n",
    "    logging.info(\"No New Files found from source\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark=SparkSession.builder.appName(\"test\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "for files in newSearchesFiles:\n",
    "    ts=util.getTsFromFileName(files)\n",
    "    spark.read.json(f\"{paths.srcSearches}/{files}\").coalesce(1).write.options(header=\"True\",compression=\"snappy\").parquet(f\"{paths.rawSearches}/{ts}\")\n",
    "\n",
    "for files in newVisitorFiles:\n",
    "    ts=util.getTsFromFileName(files)\n",
    "    spark.read.json(f\"{paths.srcVisitors}/{files}\").coalesce(1).write.options(header=\"True\",compression=\"snappy\").parquet(f\"{paths.rawVisitors}/{ts}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TASK 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "visitorStruct = StructType([\n",
    "    StructField(\"countPerday\", StringType()),\n",
    "    StructField(\"country\", StringType()),\n",
    "    StructField(\"first_hit_pagename\", StringType()),\n",
    "    StructField(\"hits_avg\", StringType()),\n",
    "    StructField(\"logged_in\", StringType()),\n",
    "    StructField(\"region\", StringType()),\n",
    "    StructField(\"registered\", StringType()),\n",
    "    StructField(\"visit_start\", StringType()),\n",
    "    StructField(\"visitor_id\", StringType()),\n",
    "    StructField(\"visits\", StringType())\n",
    "])\n",
    "\n",
    "def dateHandler(dateStr:str)->str:\n",
    "    year=dateStr.split(\"-\")[0]\n",
    "    if(len(year)==2):\n",
    "        dateStr=\"20\"+dateStr\n",
    "    return dateStr\n",
    "\n",
    "udateHandler=udf(dateHandler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanVisitor(df:DataFrame)->DataFrame:\n",
    "    df=df\\\n",
    "    .withColumn(\"hits_avg\",df[\"hits_avg\"].cast(IntegerType()))\\\n",
    "    .withColumn(\"logged_in\",df[\"logged_in\"].cast(BooleanType()))\\\n",
    "    .withColumn(\"visit_start\", udateHandler(df.visit_start) )\\\n",
    "    .withColumn(\"visits\",df.visits.cast(IntegerType()))\n",
    "    \n",
    "    df=df\\\n",
    "    .withColumn(\"visit_start\", to_timestamp(df.visit_start, \"yyyy-MM-dd HH:mm:ss\"))\n",
    "\n",
    "    df=df\\\n",
    "    .withColumn(\"date\", date_format(df.visit_start,\"yyyy-MM-dd\").cast(DateType()))\\\n",
    "    .withColumn(\"visitor_id\",df.visitor_id.cast(LongType()))\n",
    "\n",
    "    df=df.na.fill(0,[\"visitor_id\"])\n",
    "    return df\n",
    "\n",
    "def cleanSearches(df:DataFrame)->DataFrame:\n",
    "    df= df\\\n",
    "    .withColumn(\"date_time\",to_timestamp(df.date_time, \"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\"))\\\n",
    "    .withColumn(\"visitor_id\", when(trim(df.visitor_id) == \"null\",lit(0) ).otherwise(df.visitor_id))\n",
    "\n",
    "    df=df.na.fill(\"0\",[\"visitor_id\"])\\\n",
    "    .withColumn(\"date\", date_format(df.date_time,\"yyyy-MM-dd\").cast(DateType()))\n",
    "\n",
    "    df=df.withColumn(\"visitor_id\",df.visitor_id.cast( LongType() ))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root: rawVisitorPartitions = 2 , rawVisitorDF.count=29997 \n",
      "INFO:root: rawSearchesPartitions=2 , rawSearchesPartitions =26395\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[date_time: string, destination_out: string, destination_ret: string, flight_date_inbound: string, flight_date_outbound: string, origin_out: string, origin_ret: string, segments: bigint, visitor_id: double]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawVisitorDF= spark.read.options(header=\"True\").options(inferSchema=\"True\").parquet(f\"{paths.rawVisitors}/*\").cache()\n",
    "logging.info(f\" rawVisitorPartitions = {rawVisitorDF.rdd.getNumPartitions()} , rawVisitorDF.count={rawVisitorDF.count()} \")\n",
    "cleanVisitorDF=rawVisitorDF.transform(cleanVisitor)\n",
    "rawVisitorDF.unpersist()\n",
    "\n",
    "rawSearchesDF= spark.read.options(header=\"True\").options(inferSchema=\"True\").parquet(f\"{paths.rawSearches}/*\").cache()\n",
    "logging.info(f\" rawSearchesPartitions={rawSearchesDF.rdd.getNumPartitions()} , rawSearchesPartitions ={rawSearchesDF.count()}\")\n",
    "cleanSearchesDF=rawSearchesDF.transform(cleanSearches)\n",
    "rawSearchesDF.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latestVisitorDF=cleanVisitorDF.groupBy(\"visitor_id\",\"date\").agg(max(\"visit_start\").alias(\"visit_start\")).cache()\n",
    "\n",
    "latestVisitorDFExtended=latestVisitorDF\\\n",
    "                        .join(cleanVisitorDF,[\"visitor_id\",\"visit_start\"])\\\n",
    "                        .select(\"visitor_id\",\"visit_start\",latestVisitorDF.date,\"country\",\"region\")\\\n",
    "                        .withColumnRenamed(\"visit_start\",\"date_time\")\n",
    "\n",
    "\n",
    "latestVisitorDFExtended.filter(\"visitor_id=25117075546\").show(10)\n",
    "cleanSearchesDF.printSchema()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cleanVisitorDF.filter(\"visitor_id=25117075546\").orderBy(desc(cleanVisitorDF.visit_start)).show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 561:=================================================>   (187 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+------+--------+\n",
      "|      date|country|region|count(1)|\n",
      "+----------+-------+------+--------+\n",
      "|2021-01-27|   null|  null|      56|\n",
      "|2021-05-12|   null|  null|     179|\n",
      "|2021-04-24|   null|  null|     210|\n",
      "|2021-02-15|   null|  null|     185|\n",
      "|2021-05-03|   null|  null|     209|\n",
      "|2021-03-22|   null|  null|     201|\n",
      "|2021-03-19|    fra|   idf|       2|\n",
      "|2021-01-25|   null|  null|     203|\n",
      "|2021-03-07|   null|  null|     193|\n",
      "|2021-05-07|   null|  null|     206|\n",
      "|2021-04-07|   null|  null|     211|\n",
      "|2021-04-15|   null|  null|     207|\n",
      "|2021-01-05|   null|  null|     215|\n",
      "|2021-04-23|   null|  null|     178|\n",
      "|2021-02-21|   null|  null|     211|\n",
      "|2021-02-05|   null|  null|     200|\n",
      "|2021-03-21|   null|  null|     168|\n",
      "|2021-02-10|   null|  null|     187|\n",
      "|2021-01-19|   null|  null|     229|\n",
      "|2021-02-06|   null|  null|     218|\n",
      "+----------+-------+------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "cleanSearchesDF\\\n",
    ".join(latestVisitorDFExtended,[\"visitor_id\",\"date\"],\"left_outer\")\\\n",
    ".groupBy(cleanSearchesDF.date,\"country\",\"region\")\\\n",
    ".agg(count(\"*\"))\\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26395"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanSearchesDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+\n",
      "|      date|count(1)|\n",
      "+----------+--------+\n",
      "|2021-01-27|     191|\n",
      "|2021-05-12|     179|\n",
      "|2021-04-29|     218|\n",
      "|2021-04-24|     210|\n",
      "|2021-02-15|     185|\n",
      "|2021-03-22|     201|\n",
      "|2021-05-03|     209|\n",
      "|2021-01-18|     197|\n",
      "|2021-01-25|     203|\n",
      "|2021-02-02|     202|\n",
      "|2021-04-25|     213|\n",
      "|2021-03-07|     193|\n",
      "|2021-05-07|     206|\n",
      "|2021-04-21|     197|\n",
      "|2021-02-26|     168|\n",
      "|2021-04-07|     211|\n",
      "|2021-04-15|     207|\n",
      "|2021-05-09|     209|\n",
      "|2021-01-05|     215|\n",
      "|2021-04-23|     178|\n",
      "+----------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleanSearchesDF.groupBy(\"date\").agg(count(\"*\")).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
